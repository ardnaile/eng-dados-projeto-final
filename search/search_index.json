{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto Engenharia de Dados","text":"<p>Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto de Engenharia de Dados do curso de Engenharia de Software da SATC. Este projeto visa construir uma pipeline de dados para uma loja de carros utilizando ferramentas como Python, PySpark, Delta Lake, MongoDB, e Azure Data Lake.</p>"},{"location":"#estrutura-do-projeto","title":"Estrutura do Projeto","text":"<p>Abaixo est\u00e3o as principais se\u00e7\u00f5es da documenta\u00e7\u00e3o:</p> <ul> <li>Pr\u00e9 requisitos</li> <li>Sistema de Origem</li> <li>Cria\u00e7\u00e3o do Azure Data Lake Storage Gen 2</li> <li>Scripts de Transforma\u00e7\u00e3o</li> </ul>"},{"location":"#equipe","title":"Equipe","text":"<ul> <li>Eliandra Cardoso - Banco MongoDB e documenta\u00e7\u00e3o</li> <li>Gustavo Valsechi - Extra\u00e7\u00e3o e transforma\u00e7\u00e3o dos dados</li> <li>Paulo Dal Ponte - Cria\u00e7\u00e3o do Data Lake e dashboard</li> </ul>"},{"location":"criacao-azure-dls/","title":"Cria\u00e7\u00e3o do Azure Data Lake Storage Gen 2","text":"<p>Para cria\u00e7\u00e3o do Data Lake com Terraform, os pr\u00e9-requisitos s\u00e3o:</p> <ul> <li>Azure CLI</li> <li>Visual Studio Code</li> <li>Terraform</li> <li>Conta Azure</li> </ul>"},{"location":"criacao-azure-dls/#configuracao-do-azure-e-databricks","title":"Configura\u00e7\u00e3o do Azure e Databricks","text":"<ol> <li> <p>Logar na conta:</p> <pre><code>Az login\n</code></pre> </li> <li> <p>Validar assinatura atual:</p> <pre><code>Az account list -o table\n</code></pre> </li> <li> <p>Criar o servi\u00e7o na conta:</p> <pre><code>az ad sp create-for-rbac --name \u201c&lt;nome_desejado&gt;\u201d --role Contributor --scopes /subscriptions/&lt;nome_da_sua_subscription&gt;\n</code></pre> </li> <li> <p>Na pasta <code>iac/adls/</code>, criar o arquivo com nome <code>**terraform.tfvars**</code> e substituir os valores de <code>subscription_id</code> e <code>resource_group_name</code> pelos valores coletados no passo anterior.</p> <pre><code>subscription_id     = \"00000000-ffff-0000-0000-ffffffffffff\"\nresource_group_name = \"rg-iac-data-engineering\"\n</code></pre> </li> <li> <p>Na pasta <code>iac/databricks/</code>, criar o arquivo <code>**terraform.tfvars**</code> e inserir as vari\u00e1veis:</p> <pre><code>azure_client_id     = \"22222222-bbbb-2222-2222-bbbbbbbbbbbb\"\nazure_client_secret = \"AA.AA~FffFFfff_FFFFFfFFFFFffFf-fFfffffff\"\nazure_tenant_id     = \"11111111-aaaa-1111-1111-aaaaaaaaaaaa\"\nworkspace_name      = \"ws-iac-data-engineering\"\nsubscription_id     = \"00000000-ffff-0000-0000-ffffffffffff\"\nresource_group_name = \"rg-iac-data-engineering\"\n</code></pre> </li> <li> <p>Ap\u00f3s criar os arquivos, na pasta <code>iac/databricks/</code>, execute os seguintes comandos:</p> <pre><code>Terraform init\nTerraform validate\nTerraform fmt\nTerraform plan\nTerraform apply\n</code></pre> </li> <li> <p>Ap\u00f3s a execu\u00e7\u00e3o, acesse o portal Azure e valide a cria\u00e7\u00e3o da conta Databricks acessando o componente Azure Databricks conforme as imagens abaixo:</p> </li> </ol> <p></p> <p></p> <p></p> <p>Observa\u00e7\u00e3o: Ap\u00f3s o uso, execute o comando <code>Terraform destroy</code> para evitar cobran\u00e7as indevidas. ATEN\u00c7\u00c3O: esse comando destr\u00f3i todos os recursos criados anteriormente!</p>"},{"location":"criacao-azure-dls/#criacao-do-adls-no-azure-com-terraform","title":"Cria\u00e7\u00e3o do adls no Azure com Terraform","text":"<ol> <li>Com o arquivo <code>**terraform.tfvar**</code> j\u00e1 criado, navegar at\u00e9 o diret\u00f3rio <code>iac\\adls\\</code> e executar os comandos para o Terraform:</li> </ol> <pre><code>Terraform init  \nTerraform validate\nTerraform fmt\nTerraform plan\nTerraform apply\n</code></pre> <p>Ap\u00f3s a cria\u00e7\u00e3o, acessar o Portal Azure e validar a cria\u00e7\u00e3o no componente Contas de Armazenamento. Devem ter sido criadas as camadas landing, bronze, silver e gold.</p> <p></p> <p></p> <p></p> <p></p>"},{"location":"criacao-azure-dls/#criacao-da-chave-de-acesso-ao-adls","title":"Cria\u00e7\u00e3o da chave de acesso ao adls","text":"<ol> <li>Acessar aba seguran\u00e7a&gt; Assinatura de acesso partilhado</li> </ol> <ol> <li>Marcar as 3 op\u00e7\u00f5es de Recursos Permitidos</li> <li>Alterar a data fim se necess\u00e1rio</li> <li>Gerar chave</li> </ol> <p>Logo ap\u00f3s estar\u00e1 dispon\u00edvel o Access Token para uso de conex\u00f5es externas ou scripts criados posteriormente</p>"},{"location":"pre-requisitos/","title":"Pr\u00e9 requisitos","text":"<p>Antes de executar o projeto, \u00e9 necess\u00e1rio instalar as depend\u00eancias do ambiente. Para isso, siga os passos abaixo:</p> <ol> <li> <p>Tenha o Python instalado na vers\u00e3o 3.11.9.</p> <p>Certifique-se de que o Python est\u00e1 instalado em seu sistema. Caso n\u00e3o tenha, acesse o site python.org e siga as instru\u00e7\u00f5es para instala\u00e7\u00e3o ou utilize o gerenciador de vers\u00f5es Pyenv.</p> </li> <li> <p>Crie e ative um ambiente virtual (opcional, mas recomendado):</p> <pre><code>python -m venv venv\nsource venv/bin/activate  # No Windows: venv\\Scripts\\activate\n</code></pre> </li> <li> <p>Instale as depend\u00eancias do projeto:</p> <p>Execute o seguinte comando no terminal para instalar todos os pacotes necess\u00e1rios listados no arquivo <code>requirements.txt</code>:</p> <pre><code>pip install -r requirements.txt\n</code></pre> <p>Esse arquivo cont\u00e9m todas as bibliotecas e vers\u00f5es espec\u00edficas usadas no projeto, garantindo que o ambiente de execu\u00e7\u00e3o seja consistente.</p> </li> <li> <p>Configure as vari\u00e1veis de ambiente:</p> <p>Verifique o arquivo <code>.env.example</code> e preencha as vari\u00e1veis necess\u00e1rias, renomeando-o para <code>.env</code>. Este arquivo armazena informa\u00e7\u00f5es sens\u00edveis, como credenciais de banco de dados ou chaves de API.</p> </li> </ol> <p>Ap\u00f3s concluir esses passos, voc\u00ea estar\u00e1 pronto para executar o projeto!</p>"},{"location":"scripts-transformacao/","title":"Scripts de Transforma\u00e7\u00e3o","text":"<p>Os notebooks utilizados neste projeto est\u00e3o no diret\u00f3rio <code>scripts</code> e representam cada etapa do pipeline de dados. Abaixo est\u00e1 uma descri\u00e7\u00e3o de cada arquivo e sua fun\u00e7\u00e3o:</p> <ol> <li><code>extraction_to_landing.ipynb</code>: realiza a extra\u00e7\u00e3o de dados do CSV e os grava na Landing Zone no Azure Data Lake no formato JSON</li> <li><code>transform_landing_to_bronze.ipynb</code>: transforma os dados da Landing Zone gravando na camada Bronze em formato Delta Lake</li> <li><code>transform_bronze_to_silver.ipynb</code>: trata e refina os dados da camada Bronze, movendo-os para a camada Silver</li> <li><code>transform_silver_to_gold.ipynb</code>: gera um OBT na camada Gold, pronto para dashboards e consultas.</li> </ol>"},{"location":"scripts-transformacao/#ordem-de-execucao","title":"Ordem de Execu\u00e7\u00e3o","text":"<p>Para executar a pipeline corretamente, siga esta sequ\u00eancia:</p> <ol> <li>Execute o notebook <code>extraction_to_landing.ipynb</code> para carregar os dados brutos.</li> <li>Converta os dados para a camada Bronze com <code>transform_landing_to_bronze.ipynb</code>.</li> <li>Processe e otimize os dados na camada Silver com <code>transform_bronze_to_silver.ipynb</code>.</li> <li>Finalize a pipeline gerando a camada Gold com <code>transform_silver_to_gold.ipynb</code>.</li> </ol>"},{"location":"scripts-transformacao/#orquestracao-com-databricks","title":"Orquestra\u00e7\u00e3o com Databricks","text":"<p>Mais informa\u00e7\u00f5es em breve.</p>"},{"location":"sistema-de-origem/","title":"Sistema de Origem","text":"<p>O sistema de origem utilizado no projeto \u00e9 um banco de dados NoSQL, hospedado no MongoDB. Foi utilizado como refer\u00eancia o dataset Car Sales Report e a biblioteca Faker para gerar parte dos dados. </p> <p>O script <code>pre_processing_csv.ipynb</code> que foi usado no Google Colab para gerar esses dados est\u00e1 no diret\u00f3rio <code>scripts</code>.</p> <p>O banco conta com as seguintes collections:</p> <ul> <li>Cars: 10 mil registros com os campos car_id, company, model, engine, transmission, color, body, year e price</li> </ul> <p></p> <ul> <li> <p>Clients: 13 mil registros com os campos client_id, name, address, email e phone_number </p> </li> <li> <p>Employees: 100 registros com os campos employee_id, name, position e hire_date </p> </li> <li> <p>Payments: 25 mil registros com os campos payment_id, sale_id, payment_date, amount e payment_method </p> </li> <li> <p>Sales: 20 mil registros com os campos sale_id, client_id, car_id, sale_date e employee_id </p> </li> <li> <p>Services: 5 mil registros com os campos service_id, sale_id, service_type, service_date e cost </p> </li> </ul> <p>Al\u00e9m disso, todos os documentos possuem o campo <code>_id</code>, que \u00e9 o ID gerado pelo pr\u00f3prio MongoDB.</p>"}]}